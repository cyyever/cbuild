{
  "git_url": "git@github.com:Dao-AILab/flash-attention.git",
  "cache_time": 30,
  "main_language": "python",
  "new_feature": [
    "ml_lib"
  ],
  "environment_variable": [
    "MAX_JOBS=20",
    "TORCH_CUDA_ARCH_LIST=${CUDAARCHS}",
    "FLASH_ATTN_CUDA_ARCHS=${CUDAARCHS}",
    "FLASH_ATTENTION_FORCE_CXX11_ABI=TRUE",
    "run_test=0",
    "CC=/usr/bin/gcc-14",
    "CXX=/usr/bin/g++-14",
    "CUDAHOSTCXX=/usr/bin/g++-14"
  ]
}
